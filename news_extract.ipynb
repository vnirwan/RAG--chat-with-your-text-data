{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1989bebd-f750-45d7-b9f1-c93de89c782f",
   "metadata": {},
   "source": [
    "Notebook to scrape news article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3585d818-abc8-4218-98dd-7a57c342a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = r'your_dir'\n",
    "\n",
    "# Change the current working directory to the specified directory\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74138c0c-9eea-4e05-a532-61f21dc1c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install newspaper3k lxml[html_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9038619d-9aab-4e8e-99f0-bd3080d0aaf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 newspaper3k lxml[html_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "717fc3ba-df4d-4ddc-94b4-1b6f73233d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles found: 547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m links \u001b[38;5;241m=\u001b[39m fetch_article_links(news_url)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal articles found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m get_articles_from_links(links, output_file, keywords, start_date, end_date)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered articles have been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 35\u001b[0m, in \u001b[0;36mget_articles_from_links\u001b[1;34m(links, output_file, keywords, start_date, end_date)\u001b[0m\n\u001b[0;32m     33\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(link)\n\u001b[0;32m     34\u001b[0m article\u001b[38;5;241m.\u001b[39mset_html(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 35\u001b[0m article\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m     36\u001b[0m article_date \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mpublish_date\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\newspaper\\article.py:207\u001b[0m, in \u001b[0;36mArticle.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m document_cleaner \u001b[38;5;241m=\u001b[39m DocumentCleaner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m    205\u001b[0m output_formatter \u001b[38;5;241m=\u001b[39m OutputFormatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m--> 207\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mget_title(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_title(title)\n\u001b[0;32m    210\u001b[0m authors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mget_authors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\newspaper\\extractors.py:267\u001b[0m, in \u001b[0;36mContentExtractor.get_title\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# title from h1\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# - extract the longest text from all h1 elements\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# - too short texts (fewer than 2 words) are discarded\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# - clean double spaces\u001b[39;00m\n\u001b[0;32m    266\u001b[0m title_text_h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 267\u001b[0m title_element_h1_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mgetElementsByTag(doc,\n\u001b[0;32m    268\u001b[0m                                                      tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    269\u001b[0m title_text_h1_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mgetText(tag) \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    270\u001b[0m                       title_element_h1_list]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title_text_h1_list:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# sort by len and set the longest\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\newspaper\\parsers.py:123\u001b[0m, in \u001b[0;36mParser.getElementsByTag\u001b[1;34m(cls, node, tag, attr, value, childs, use_regex)\u001b[0m\n\u001b[0;32m    121\u001b[0m         trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate(@\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (attr, string\u001b[38;5;241m.\u001b[39mascii_uppercase, string\u001b[38;5;241m.\u001b[39mascii_lowercase)\n\u001b[0;32m    122\u001b[0m         selector \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m[contains(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (selector, trans, value\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m--> 123\u001b[0m elems \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mxpath(selector, namespaces\u001b[38;5;241m=\u001b[39mNS)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# remove the root node\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# if we have a selection tag\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m elems \u001b[38;5;129;01mand\u001b[39;00m (tag \u001b[38;5;129;01mor\u001b[39;00m childs):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def fetch_article_links(news_url):\n",
    "    response = requests.get(news_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if link.startswith('http'):\n",
    "            links.append(link)\n",
    "        elif link.startswith('/'):\n",
    "            links.append(news_url + link)\n",
    "    return links\n",
    "\n",
    "def get_articles_from_links(links, output_file, keywords, start_date, end_date):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for link in links:\n",
    "            if \"cnn-underscored\" in link or \"cnn.com\" not in link:\n",
    "                continue  # Skip problematic sections or invalid links\n",
    "            try:\n",
    "                # Use requests to get the article content\n",
    "                response = requests.get(link, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Parse the article with newspaper\n",
    "                article = Article(link)\n",
    "                article.set_html(response.text)\n",
    "                article.parse()\n",
    "                article_date = article.publish_date\n",
    "\n",
    "                if article_date is None:\n",
    "                    continue\n",
    "                if article_date.tzinfo is not None:\n",
    "                    article_date = article_date.replace(tzinfo=None)\n",
    "                if not (start_date <= article_date <= end_date):\n",
    "                    continue\n",
    "                if not any(keyword.lower() in article.text.lower() for keyword in keywords):\n",
    "                    continue\n",
    "\n",
    "                file.write(f\"Title: {article.title}\\n\")\n",
    "                file.write(f\"URL: {article.url}\\n\")\n",
    "                file.write(f\"Date: {article_date.strftime('%Y-%m-%d')}\\n\")\n",
    "                file.write(f\"Text: {article.text}\\n\\n\")\n",
    "                print(f\"Processed article: {link}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process article: {link}, Error: {str(e)}\")\n",
    "            time.sleep(1)  # Add a delay to prevent rate limiting\n",
    "\n",
    "news_url = 'http://cnn.com'\n",
    "output_file = 'filtered_news_articles.txt'\n",
    "keywords = [\"OpenAI\", \"ChatGPT\", \"GPT-3\", \"GPT-4\"]\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 7, 31)\n",
    "\n",
    "links = fetch_article_links(news_url)\n",
    "print(f\"Total articles found: {len(links)}\")\n",
    "\n",
    "get_articles_from_links(links, output_file, keywords, start_date, end_date)\n",
    "\n",
    "print(f\"Filtered articles have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597de1f-dff1-451d-bb8c-cb54aa2de266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
